## 经典模型介绍

## ResNet 

- 核心思想：**加的层只是一种身份映射，而其它层只是更浅层的复制（the added layers are identity mapping, and the other layers are copied from the learned shallower model.）**
- ![img](https://img-blog.csdnimg.cn/20210702073617262.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RyZWFzdXJlYXNoZXM=,size_16,color_FFFFFF,t_70)
- 其中ResNet提出了两种mapping：一种是identity mapping，指的就是图1中”弯弯的曲线”，另一种residual mapping，指的就是除了”弯弯的曲线“那部分，所以最后的输出是 y=F(x)+x。identity mapping顾名思义，就是指本身，也就是公式中的x，而residual mapping指的是“差”，也就是y−x，所以残差指的就是F(x)部分。
- shortcut connection：如图的连接。



## MobileNet V2

- 深度可分离卷积

  - 深度卷积：

    ![img](https://pic2.zhimg.com/80/v2-b74a5e8241eb500949d8dcc47558d035_720w.jpg)

    将**卷积核拆分成为但单通道形式**，在**不改变输入特征图像的深度**的情况下，对**每一通道进行卷积操作**，这样就得到了**和输入特征图通道数一致的输出特征图**。

  - 逐点卷积:

    ![img](https://pic1.zhimg.com/80/v2-f480c4453e9b7915c88d34ca79288e20_720w.jpg)

    **逐点卷积就是1×1卷积**。主要作用就是对特征图进行升维和降维

  - 与标准卷积对比：

    ![img](https://pic3.zhimg.com/80/v2-e123df730cbb163fff15987638bfb03e_720w.jpg)

- RELU6

  ![img](https://pic3.zhimg.com/80/v2-9f1a722b2eceaa84169300521843bdfa_720w.jpg)

  上图左边是普通的ReLU，对于大于0的值不进行处理，右边是ReLU6，当输入的值大于6的时候，返回6，relu6“具有一个边界”。作者认为**ReLU6作为非线性激活函数，在低精度计算下具有更强的鲁棒性**。(比如模型量化成8-bit)

- Linear bottleneck：最后的那个ReLU6换成Linear。

  ![img](https://pic4.zhimg.com/80/v2-1670b59e3e783af66eb4e662f68218ef_720w.jpg)

- Inverted residuals：

  ![img](https://pic2.zhimg.com/80/v2-7b4422a0efcc89c46029052d29d538b5_720w.jpg)

  - ResNet 先降维 (0.25倍)、卷积、再升维。
  - MobileNetV2 则是 先升维 (6倍)、卷积、再降维；刚好V2的block刚好与Resnet的block相反

- https://zhuanlan.zhihu.com/p/70703846

## PSPNet

- 金字塔池化模型(Pyramid Pooling Module)


![img](https://pic1.zhimg.com/80/v2-3dcb678fb6831d39549d876366329ac4_720w.jpg)

相比于FCN，PPM更加有效的获取的全局上下文先验

- ​	https://zhuanlan.zhihu.com/p/403468761

### DeeplabV3

## MaskRcnn

## CellPose

