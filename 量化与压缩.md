# Cuda

# TensorRt

# 量化

## 量化目的

- 量化方法的目的就是使用 8 位或 16 位的整型数来替代浮点数，这种方法试图利用定点点积来替代浮点点积，这很大程度上降低了神经网络在无硬浮点设备上的运算开销。

- 量化方法是一种类似于离差标准化的归一化方法，是对原始数据进行线性变换，使结果映射到一定范围内，具体公式如下：

  - ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+V_q+%3D+Q+%2A+%28V_x+-+min%28V_x%29%29+%5C%5D)

  - ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+V_x%27+%3D+V_q+%2F+Q+%2B+min%28V_x%29+%5C%5D)
  -  ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+Q%3DS%2FR+%5C%5D), ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+R%3Dmax%28V_x%29-min%28V_x%29+%5C%5D) , ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+S+%3D+1+%3C%3C+bits+-+1+%5C%5D)

- https://zhuanlan.zhihu.com/p/38328685

## Pytorch提供了三种量化模型的方法：

- 训练后动态量化(Post Training Dynamic Quantization)：最简单的量化形式，权重被提前量化，激活在推理过程中被动态量化

  ```python
  import torch
  
  # define a floating point model
  class M(torch.nn.Module):
      def __init__(self):
          super(M, self).__init__()
          self.fc = torch.nn.Linear(4, 4)
  
      def forward(self, x):
          x = self.fc(x)
          return x
  
  # create a model instance
  model_fp32 = M()
  # create a quantized model instance
  model_int8 = torch.quantization.quantize_dynamic(
      model_fp32,  # the original model
      {torch.nn.Linear},  # a set of layers to dynamically quantize
      dtype=torch.qint8)  # the target dtype for quantized weights
  
  # run the model
  input_fp32 = torch.randn(4, 4, 4, 4)
  res = model_int8(input_fp32)
  ```

- 训练后静态量化(Post Training Static Quantization)：最常用的量化形式，权重提前量化，并且基于观察校准过程中模型的行为来预先计算激活张量的比例因子和偏差。

  ```python
  import torch
  
  # define a floating point model where some layers could be statically quantized
  class M(torch.nn.Module):
      def __init__(self):
          super(M, self).__init__()
          # QuantStub converts tensors from floating point to quantized
          self.quant = torch.quantization.QuantStub()
          self.conv = torch.nn.Conv2d(1, 1, 1)
          self.relu = torch.nn.ReLU()
          # DeQuantStub converts tensors from quantized to floating point
          self.dequant = torch.quantization.DeQuantStub()
  
      def forward(self, x):
          # manually specify where tensors will be converted from floating
          # point to quantized in the quantized model
          x = self.quant(x)
          x = self.conv(x)
          x = self.relu(x)
          # manually specify where tensors will be converted from quantized
          # to floating point in the quantized model
          x = self.dequant(x)
          return x
  
  # create a model instance
  model_fp32 = M()
  
  # model must be set to eval mode for static quantization logic to work
  model_fp32.eval()
  
  # attach a global qconfig, which contains information about what kind
  # of observers to attach. Use 'fbgemm' for server inference and
  # 'qnnpack' for mobile inference. Other quantization configurations such
  # as selecting symmetric or assymetric quantization and MinMax or L2Norm
  # calibration techniques can be specified here.
  model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')
  
  # Fuse the activations to preceding layers, where applicable.
  # This needs to be done manually depending on the model architecture.
  # Common fusions include `conv + relu` and `conv + batchnorm + relu`
  model_fp32_fused = torch.quantization.fuse_modules(model_fp32, [['conv', 'relu']])
  
  # Prepare the model for static quantization. This inserts observers in
  # the model that will observe activation tensors during calibration.
  model_fp32_prepared = torch.quantization.prepare(model_fp32_fused)
  
  # calibrate the prepared model to determine quantization parameters for activations
  # in a real world setting, the calibration would be done with a representative dataset
  input_fp32 = torch.randn(4, 1, 4, 4)
  model_fp32_prepared(input_fp32)
  
  # Convert the observed model to a quantized model. This does several things:
  # quantizes the weights, computes and stores the scale and bias value to be
  # used with each activation tensor, and replaces key operators with quantized
  # implementations.
  model_int8 = torch.quantization.convert(model_fp32_prepared)
  
  # run the model, relevant calculations will happen in int8
  res = model_int8(input_fp32)
  ```

- 量化意识训练：在极少数情况下，训练后量化不能提供足够的准确性，可以使用torch.quantization.FakeQuantize 函数通过模拟量化来进行训练。

# 模型压缩

https://zhuanlan.zhihu.com/p/395228490